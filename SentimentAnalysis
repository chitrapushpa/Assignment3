%reload_ext autoreload
%autoreload 2
%matplotlib inline

from fastai.learner import *

import torchtext
from torchtext import vocab, data
from torchtext.datasets import language_modeling

from fastai.rnn_reg import *
from fastai.rnn_train import *
from fastai.nlp import *
from fastai.lm_rnn import *

import dill as pickle
import spacy

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)


import seaborn as sns
import matplotlib.pyplot as plt

reviews = pd.read_csv("../input/Womens Clothing E-Commerce Reviews.csv")
reviews.head()

reviews.info()

review_text = reviews[['Review Text', 'Rating']]
review_text.head()

split = np.random.randn(len(review_text)) <0.8
TRN = review_text[split]
TEST = review_text[~split]
print("Total rows in train:" ,len(TRN),"and test:", len(TEST))

spacy_tok = spacy.load('en')

' '.join([sent.string.strip() for sent in spacy_tok(TRN['Review Text'][4])])

TEXT = data.Field(lower=True, tokenize="spacy")

bs=64; bptt=70

FILES =dict(train_df=TRN, val_df=TEST, test_df=TEST,col="Review Text")
md = LanguageModelData.from_dataframes(PATH, TEXT, **FILES,bs=bs,bptt=bptt, min_freq=10)

len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)

TEXT.vocab.itos[:12]

TEXT.vocab.stoi['ugly']

md.trn_ds[0].text[:50]

TEXT.numericalize([md.trn_ds[0].text[:50]])

next(iter(md.trn_dl))

em_sz = 200
nh = 500
nl =3

opt_fn = partial(optim.Adam, betas=(0.7, 0.99))


LEARNER_KWARGS = [
    'tmp_name', 'models_name', 'metrics', 'clip', 'crit',
]

def get_model(self, opt_fn, emb_sz, n_hid, n_layers, **kwargs):
    lm_kwargs = {k:v for k,v in kwargs.items() if k not in LEARNER_KWARGS}
    m = get_language_model(self.nt, emb_sz, n_hid, n_layers, self.pad_idx, **lm_kwargs)
    model = SingleModel(to_gpu(m))
    learner_kwargs = {k:v for k,v in kwargs.items() if k in LEARNER_KWARGS}
    return RNN_Learner(self, model, opt_fn=opt_fn, **learner_kwargs)

LanguageModelData.get_model = get_model

learner = md.get_model(opt_fn, em_sz, nh, nl,
                      dropouti=0.05,dropout=0.05, wdrop=0.1,dropoute=0.02,dropouth=0.05,
                      tmp_name=TMP_PATH,models_name=MODELS_PATH)
learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)
learner.clip=0.3

learner


learner.fit(3e-3,2,wds=1e-6,cycle_len=1, cycle_mult=2)

math.exp(3.495)

learner.save_encoder('adam1_enc')

learner.load_encoder('adam1_enc')

learner.fit(3e-3,1,wds=1e-6,cycle_len=5)

math.exp(3.275)

learner.save_encoder('adam1_enc')
learner.load_encoder('adam1_enc')

